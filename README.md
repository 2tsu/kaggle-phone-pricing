# 重回帰における特徴量選択アルゴリズム

重回帰分析では、モデルの性能を向上させるために適切な特徴量を選択することが重要です。以下は、重回帰分析で使用される様々な特徴量選択アルゴリズムの概要です。

## ラッパー法

ラッパー法（Wrapper Method）は、特定の機械学習アルゴリズムの性能を基準にして特徴量を選択する方法です。この方法では、特徴量のサブセットを繰り返し評価し、モデルの性能が最も良くなる特徴量の組み合わせを選択します。

## 後退選択

後退選択（Backward Elimination）は、全ての特徴量を含むモデルから開始し、逐次的に特徴量を削除していく方法です。各ステップで削除する特徴量は、モデルの性能に対する影響が最も小さいものを選択します。

## p値を用いた選択

p値を用いた選択（p-value Based Selection）は、各特徴量の統計的有意性を評価し、p値が一定の閾値以下の特徴量のみを選択する方法です。この方法では、統計的に有意でない特徴量を除外することで、モデルの精度を向上させることができます。

## ランダムフォレストを用いた選択（開発中）

ランダムフォレストを用いた選択（Random Forest Based Selection）は、ランダムフォレストの重要度指標を使用して特徴量を選択する方法です。この方法では、ランダムフォレストモデルを構築し、各特徴量の重要度スコアに基づいて特徴量を選択します。（現在開発中）

## 前進選択

前進選択（Forward Selection）は、空のモデルから開始し、逐次的に特徴量を追加していく方法です。各ステップで追加する特徴量は、モデルの性能が最も向上するものを選択します。

## 相関係数を用いた選択

相関係数を用いた選択（Correlation Coefficient Based Selection）は、特徴量間の相関関係を評価し、高い相関を持つ特徴量を除外する方法です。この方法では、冗長な特徴量を除外することでモデルの性能を向上させることができます。

# ディレクトリ構造

プロジェクトのディレクトリ構造と各ファイルの説明は以下の通りです。

kaggle-phone-pricing/
├── plane.py
│ └── すべてのカラムを特徴量とした
├── method_wrapper_backward_1.py
│ └── p-value > 0.05 の特徴量を削除するラッパー法
├── method_wrapper_backward_1_1.py
│ └── p-value > 0.05 の特徴量を削除するラッパー法
│ 予測価格帯を10,0000以下で訓練したもの
├── method_wrapper_backward_2.py
│ └── ランダムフォレストを用いて特徴量を削除するラッパー法
│ 開発中
├── method_wrapper_forward_1.py
│ └── p-value > 0.05 の特徴量を削除する前進選択ラッパー法
└── method_filter.py
└── 相関係数を用いたフィルタ法による特徴量選択